# Data

## Sources

The dataset used in this project is the 2020 Remote Working Survey, part of the NSW Remote Working Survey series, publicly available through the New South Wales Government’s Open Data Portal. “Data source: [Remote working survey 2020](https://data.nsw.gov.au/data/dataset/nsw-remote-working-survey)” The 2020 survey was conducted during August and September 2020 and aimed to understand workers experiences and attitudes toward remote and hybrid working following the first phase of the COVID-19 pandemic to study the impact of remote work on professional and personal well-being.

To be eligible, respondents had to:

-   be employed NSW residents

-   have experience of remote working in their current job. After excluding unemployed individuals and those whose occupations cannot be performed remotely (dentists, cashiers, cleaners), the sample represents approximately 59% of NSW workers.

This dataset is the 2020 wave of the survey. A second wave was collected in March and April 2021. For the present analysis, only the 2020 dataset is being processed and explored. Once the analysis of this wave is finalized, it will be possible to extend the project by including and comparing the 2021 dataset to identify how remote work patterns evolved over time.

## Description

The cleaned 2020 NSW Remote Working Survey dataset contains four main types of variables: categorical, ordinal, numeric, and binary.

-   The categorical variables describe qualitative characteristics of respondents and their employment context, such as Industry, Job_type, Organisation_Size, Household, and Years_in_job. These variables are stored as text and provide context for grouping and comparison across sectors or demographic profiles.

-   The ordinal variables represent ordered responses on Likert scales, reflecting opinions and perceptions about remote working. Variables such as Org_encouraged_remote_last_year, Collaboration_remote_last_year, Org_encouraged_remote_3_months, and Collaboration_remote_3_months are encoded numerically from 1 (“Strongly disagree”) to 5 (“Strongly agree”), allowing for quantitative analysis of attitudes.

-   The numeric variables capture measurable quantities including time allocation, remote work proportions, and productivity. Examples include Age, Remote_pct_last_year, Preferred_remote_last_year, Productivity_remote_vs_workplace, and several variables representing hours spent on commuting, working, and personal or domestic activities. These are stored as integers or floats, making them suitable for descriptive statistics and correlation analysis.

-   The binary variables (Gender, Managing_position) indicate Male/Femal or Yes/No conditions, encoded as 0 and 1. These variables enable comparisons between distinct groups.

The dataset also contains several important pieces of metadata that ensure its quality and usability for analysis. Each observation is identified by a unique respondent code (Response_ID), which guarantees traceability and prevents duplication during data processing.

In addition, all variable names were standardized to concise, descriptive identifiers (Org_encouraged_remote_last_year) to make the variables easier to read, reuse, and analyze in statistical software. This naming convention makes the analysis process simpler and clearer throughout the project.

::: callout-note
## Dataset Overview Template

-   **File used:** 2020_rws-updated.csv

-   **Format:** CSV (comma-separated values)

-   **Encoding:** latin-1 (ISO-8859-1) (Remark: when loading the dataset in Python (Google Colab), attempting to read with utf-8 caused a UnicodeDecodeError due to typographic apostrophes and special characters. The correct parameter encoding latin1 was required to successfully load the data.)

-   **Memory usage:** approximately 7.46 MB

-   **Number of observations:** 1507 respondents (1370 rows after cleaning)

-   **Number of variables:** 73 columns (25 after cleaning)

-   **Time period:** August and September 2020

-   **Geographic coverage:** New South Wales, Australia

-   **Key variables:** Gender, Age, Job_type, Organisation_Size, Managing_position, Remote_pct_last_year, Preferred_remote_last_year, Org_encouraged_remote_last_year, Collaboration_remote_last_year, Productivity_remote_vs_workplace
:::

## Loading Data

Following best practices, the file is loaded using a relative path via project_root, ensuring that the document remains fully reproducible regardless of the execution location. Because the original file contained specific typographic characters that caused decoding issues during import, the dataset is read using the latin-1 encoding.

After loading the file, several checks were performed to confirm correct import: verification of the dataset dimensions, inspection of column names and data types (df.info()), preview of the first rows to ensure values were properly formatted. These steps guarantee that the dataset is correctly imported and ready for subsequent exploratory analysis.

```{python}
#| label: load-data
# Code visibility controlled by format settings in report.qmd
#| output: true

import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import plotly as py
import plotly.graph_objs as go
import statistics

url = "https://data.nsw.gov.au/data/dataset/4ff1a0de-9bff-4b45-9f93-5a9bf44df257/resource/5b843384-5aea-4bc4-9935-0dd91ea2559d/download/2020_rws-updated.csv"

# Load the data
df = pd.read_csv(url, encoding="ISO-8859-1")

# Basic data inspection
print(f"Dataset shape: {df.shape[0]} rows × {df.shape[1]} columns")
print(f"\nData types:\n{df.dtypes}")

# Display first few rows
print("\nFirst 5 rows:")
display(df.head())
```

## Wrangling

### General Transformations

Several preprocessing and wrangling steps were performed to prepare the dataset for analysis. Before detailing each transformation.

All preprocessing steps described below are fully reproducible and validated. Each transformation relies on deterministic Python operations (such as replace(), rename(), astype(), or simple arithmetic), meaning that re-running the same code on the raw dataset will always produce identical results. After every transformation, checks such as head(), value_counts(), or describe() were used to validate that the modifications were correctly applied and that the resulting values were coherent (age ranges, Likert scales, or percentage conversions).

**Variable Duplicated**

Before any transformation, it is essential to verify that each observation in the dataset is unique. Duplicate rows can bias the analysis by over representing certain respondents or records. Identifying and removing them ensures data integrity.

This operation can be rerun on the raw dataset at any stage of the workflow, guaranteeing consistent detection of duplicated records.
The command returned an empty DataFrame, confirming that no duplicate rows were present. Therefore, no observations were removed in this step.

```{python}
#| label: drop column
# Code visibility controlled by format settings in report.qmd
#| output: true

# delete all the duplicated rows
df[df.duplicated(keep=False)]

# delete all the columns with think are not significant to lower the size of the dataset for better readibility
cols_to_drop = [0,4, 6, 11, 14, 15, 16, 18, 22, 23, 24, 26, 28, 29, 30, 31] + list(range(41, 73))
df = df.drop(df.columns[cols_to_drop], axis=1)

```

**Rename Columns**

Renaming the original survey questions into shorter and clearer variable names was necessary to improve readability and make the dataset easier to work with.

```{python}
#| label: rename column
# Code visibility controlled by format settings in report.qmd
#| output: true

rename_map = {
    "Response ID": "Response_ID",
    "What year were you born?": "Age",
    "What is your gender?": "Gender",
    "Which of the following best describes your industry?": "Industry",
    "Which of the following best describes your current occupation?": "Job_type",
    "How many people are currently employed by your organisation?": "Organisation_Size",
    "Do you manage people as part of your current occupation?": "Managing_position",
    "Which of the following best describes your household?": "Household",
    "How long have you been in your current job?": "Years_in_job",
    "Thinking about your current job, how much of your time did you spend remote working last year?": "Remote_pct_last_year",
    "Thinking about remote working last year, how strongly do you agree or disagree with the following statements? - My organisation encouraged people to work remotely": "Org_encouraged_remote_last_year",
    "Thinking about remote working last year, how strongly do you agree or disagree with the following statements? - I could easily collaborate with colleagues when working remotely": "Collaboration_remote_last_year",
    "How much of your time would you have preferred to work remotely last year?": "Preferred_remote_last_year",
    "Thinking about your current job, how much of your time did you spend remote working in the last 3 months?": "Remote_pct_last_3_months",
    "Thinking about remote working in the last 3 months, how strongly do you agree or disagree with the following statements? - My organisation encouraged people to work remotely": "Org_encouraged_remote_3_months",
    "Thinking about remote working in the last 3 months, how strongly do you agree or disagree with the following statements? - I could easily collaborate with colleagues when working remotely": "Collaboration_remote_3_months",
    "How much of your time would you have preferred to work remotely in the last 3 months?": "Preferred_remote_3_months",
    "This question is about your productivity. Productivity means what you produce for each hour that you work. It includes the amount of work you achieve each hour, and the quality of your work each hour.  \nPlease compare your productivity when you work remotely to when you work at your employer\x92s workplace.  \nRoughly how productive are you, each hour, when you work remotely?": "Productivity_remote_vs_workplace",
    "On a day when you attend your employer's workplace, how many hours would you spend doing the following activities? - Preparing for work and commuting": "Commute_hours_workplace",
    "On a day when you attend your employer's workplace, how many hours would you spend doing the following activities? - Working": "Working_hours_workplace",
    "On a day when you attend your employer's workplace, how many hours would you spend doing the following activities? - Personal and family time": "Personal_family_hours_workplace",
    "On a day when you attend your employer's workplace, how many hours would you spend doing the following activities? - Caring and domestic responsibilities": "Domestic_hours_workplace",
    "On a day when you do remote work, how many hours would you spend doing the following activities? - Preparing for work and commuting": "Commute_hours_remote",
    "On a day when you do remote work, how many hours would you spend doing the following activities? - Working": "Working_hours_remote",
    "On a day when you do remote work, how many hours would you spend doing the following activities? - Personal and family time": "Personal_family_hours_remote",
    "On a day when you do remote work, how many hours would you spend doing the following activities? - Caring and domestic responsibilities": "Domestic_hours_remote"}
df.rename(columns=rename_map, inplace=True)

```

**Standard Name Organisation Size**

The Organisation_Size variable was recoded by replacing the long original text categories with shorter, standardised. To make grouping and comparison more intuitive.

```{python}
#| label: standard name
# Code visibility controlled by format settings in report.qmd
#| output: true

df["Organisation_Size"] = df["Organisation_Size"].replace({
    "I am the only employee": "Solo business",
    "Between 1 and 4": "Micro enterprise",
    "Between 5 and 19": "Small enterprise",
    "Between 20 and 199": "Medium enterprise",
    "More than 200": "Large / Corporate enterprise"}).fillna("Other / Unspecified")
```

**Binary Variables**

The variables Gender and Managing_position, the original text responses were converted into binary categories to make them suitable for statistical analysis and group comparisons. Respondents who selected “Rather not say” for gender were removed, as this category represented only two individuals and could not form a meaningful subgroup. Gender was then encoded as 0 = Male and 1 = Female, while Managing_position was encoded as 0 = No and 1 = Yes, indicating whether the respondent supervises others. This transformation produces clean, consistent, and analysis-ready binary variables that can be easily used in descriptive statistics, visualisations, and modelling.

```{python}
#| label: binary variable
# Code visibility controlled by format settings in report.qmd
#| output: true

df = df[df["Gender"] != "Rather not say"]
df["Gender"] = df["Gender"].replace({"Male": 0, "Female": 1}).astype("category")
df["Managing_position"] = df["Managing_position"].replace({"No": 0, "Yes": 1}).astype("category")
df = df.dropna(subset=["Managing_position"])
```

**Variable Ages**

The variable Age, the dataset originally reported the respondent’s year of birth. This information was converted into a more interpretable age variable by subtracting the birth year from 2020, the year the survey was conducted. For example, a respondent born in 1985 becomes 2020 − 1985 = 35 years old. Expressing this information directly as age is more intuitive and easier to interpret in descriptive statistics, comparisons, and visualizations.

```{python}
#| label: Variable Ages
# Code visibility controlled by format settings in report.qmd
#| output: true

df["Age"] = 2020 - df["Age"]
```

**Variable Years in Job**

The variable Years_in_job, the original responses were long text categories describing tenure intervals. These were simplified into shorter and more readable labels: “5+”, “5-”, and “1-”. This transformation keeps the original meaning while making the variable easier to interpret, compare, and visualise in tables and plots.

```{python}
#| label: Variable years in job
# Code visibility controlled by format settings in report.qmd
#| output: true

df["Years_in_job"] = df["Years_in_job"].replace({
    "More than 5 years": "5+",
    "Between 1 and 5 years": "5-",
    "Between 6 and 12 months": "1-"})
```

**Variable Likert Scale Mapping**

The Likert-scale variables, this transformation allows these subjective perceptions to be analysed quantitatively. The four variables related to organisational support and collaboration were mapped using this scale. Any missing responses were replaced with the neutral value 3, corresponding to “Neither agree nor disagree”, to keep these observations in the dataset while avoiding bias from missing attitudes.

```{python}
#| label: Variable Likert Scale Mapping
# Code visibility controlled by format settings in report.qmd
#| output: true

likert_mapping = {
    "Strongly disagree": 1,
    "Somewhat disagree": 2,
    "Neither agree nor disagree": 3,
    "Somewhat agree": 4,
    "Strongly agree": 5}
likert_cols = [
    "Org_encouraged_remote_last_year",
    "Collaboration_remote_last_year",
    "Org_encouraged_remote_3_months",
    "Collaboration_remote_3_months"]
df[likert_cols] = df[likert_cols].replace(likert_mapping).fillna(3)
```

**Variables Working Remotely**

The variables describing the percentage of time spent or preferred working remotely, the original responses (“Less than 10% of my time”…“100% - All of my time”). These were first converted into numeric percentage values using a mapping dictionary. All four percentage-related variables were processed in the same way. Before applying the mapping, non-breaking spaces () and extra whitespace were removed from the strings to avoid parsing issues. After the text values were mapped to numeric percentages (“80%” → 80), the values were converted into proportions between 0 and 1 by dividing by 100.

For example: 80 becomes 0.80 50 becomes 0.50 0 becomes 0.00

This final step creates clean numerical variables that can be easily averaged, compared, or visualised in the analysis.


```{python}
#| label: Variables Working Remotely
# Code visibility controlled by format settings in report.qmd
#| output: true

remote_mapping = {
    "Rarely or never": 0,
    "Less than 10% of my time": 5,
    "10%": 10,
    "20%": 20,
    "30%": 30,
    "40%": 40,
    "50% - About half of my time": 50,
    "50% - I spent about half of my time remote working": 50,
    "60%": 60,
    "70%": 70,
    "80%": 80,
    "90%": 90,
    "100% - All of my time": 100,
    "100% - I spent all of my time remote working": 100,
    "I would not have preferred to work remotely": 0}
percent_cols = [
    "Preferred_remote_last_year",
    "Remote_pct_last_year",
    "Remote_pct_last_3_months",
    "Preferred_remote_3_months"]
for col in percent_cols:
    df[col] = df[col].astype(str).str.replace("\xa0", "", regex=True).str.strip().replace(remote_mapping)
    df[col] = pd.to_numeric(df[col], errors="coerce") / 100
```

**Variables Productivity Cleaning**

The variable Productivity_remote_vs_workplace, the survey responses (I’m 20% more productive when I work remotely” or “I’m 10% less productive”). These text responses were converted into clean numeric values using a custom parsing function. The mapping works as follows: Statements indicating more productive remotely return a positive percentage (“20% more productive” → +20). Statements indicating less productive remotely return a negative percentage (“10% less productive” → –10). Statements indicating no difference return 0. This transformation results in a numeric scale where positive values mean higher productivity when working remotely, negative values reflect lower productivity, and zero indicates no change. This allows the variable to be analysed quantitatively, averaged across groups, or used in visualisations.

```{python}
#| label: Variables Productivity Cleaning
# Code visibility controlled by format settings in report.qmd
#| output: true

def parse_productivity(val):
    if pd.isna(val):
        return None
    val = str(val).replace("\x92", "'")  # Fix apostrophe
    if "more productive" in val:
        return int(val.split("%")[0].replace("I'm ", "").strip())
    elif "less productive" in val:
        return -int(val.split("%")[0].replace("I'm ", "").strip())
    elif "same" in val:
        return 0
    return None
df["Productivity_remote_vs_workplace"] = df["Productivity_remote_vs_workplace"].apply(parse_productivity)
```


### Spotting Mistakes and Missing Data

Discuss any identified mistakes or issues with missing data and describe your approach to handling them.

::: callout-warning
## Missing Data Strategies

Different approaches for different situations:

-   **Deletion**: Remove rows/columns with missing values (when few missing)
-   **Imputation**: Fill with mean, median, mode, or advanced methods
-   **Flagging**: Create indicator variables for missingness
-   **Model-based**: Use algorithms that handle missing values

**Document your choice** and justify why it's appropriate for your data!
:::

### Listing Anomalies and Outliers

Identify any anomalies or outliers discovered, along with your approach to assessing their impact.

::: callout-tip
## Outlier Detection Methods

-   **Visual inspection**: Box plots, scatter plots
-   **Statistical methods**: Z-scores, IQR method
-   **Domain knowledge**: What values are impossible or implausible?

Remember: Not all outliers should be removed! They might be:

-   **Errors**: Data entry mistakes (should be corrected/removed)
-   **Valid extremes**: Real but unusual observations (should be kept)
-   **Key insights**: The most interesting part of your data!
:::

```{python}
#| label: data-cleaning
# Code visibility controlled by format settings in report.qmd
#| output: true

"""Example of data cleaning with pandas - proper transformations."""

import pandas as pd
import numpy as np
from IPython.display import display
from quarto_runtime import project_root

# Load raw data using the same approach as above
raw_data = project_root / "data" / "raw" / "data_raw.csv"
data = pd.read_csv(raw_data, skipinitialspace=True)

# Clean column names (lowercase, remove spaces)
data.columns = data.columns.str.strip().str.lower()

# Check for missing values
print("Missing values per column:")
print(data.isnull().sum())

# Data cleaning pipeline
data_clean = (
    data
    # Remove rows with critical missing values
    .dropna(subset=["behavior", "performance"])
    # Convert categorical variables
    .assign(
        behavior=lambda df: df["behavior"].astype("category"),
        instructor=lambda df: df["instructor"].astype("category"),
    )
    # Ensure numeric columns are proper type
    .assign(
        performance=lambda df: pd.to_numeric(df["performance"], errors='coerce')
    )
)

print(f"\nOriginal data shape: {data.shape}")
print(f"Cleaned data shape: {data_clean.shape}")
print(f"Rows removed: {data.shape[0] - data_clean.shape[0]}")

# Display cleaned data summary
print("\nCleaned data summary:")
display(data_clean.describe(include='all'))

# Save cleaned data for later use (using project_root path)
processed_path = project_root / "data" / "processed" / "data_processed.csv"
data_clean.to_csv(processed_path, index=False)
```

After cleaning, our dataset contains **`{python} data_clean.shape[0]` observations** across **`{python} data_clean.shape[1]` variables**. We removed **`{python} data.shape[0] - data_clean.shape[0]` rows** due to missing values in critical variables.

::: callout-tip
## Inline Code for Dynamic Results

Notice the inline code in the paragraph above uses Python expressions wrapped in backticks with `{python}` prefix to insert computed values directly into your text. This ensures your narrative automatically updates when data changes.

**Working examples in this document:**

-   The mean performance score is `{python} f'{data_clean["performance"].mean():.3f}'`
-   We analyzed data from `{python} data_clean['instructor'].nunique()` different instructors

**To use inline code:** Write your Python expression between backticks with the {python} prefix, like this (without the backslashes): `\{python\} your_expression_here`

This is better than hard-coding numbers, which can become outdated if you update your data!
:::

::: callout-important
## Don't Forget to Interpret!

After showing your data cleaning code, **explain your decisions**:

-   **Why** did you remove certain rows or handle missing values this way?
-   **What** impact do these choices have on your analysis?
-   **How** do your cleaning decisions align with your research questions?

Example: "We removed rows with missing performance scores (N=15, 7.5% of data) because these are our primary outcome variable and cannot be reliably imputed. This minimal data loss is acceptable and preserves the integrity of our performance analysis."
:::

::: callout-note
## Data Cleaning Checklist

Before moving to analysis, verify:

-   [x] All column names are clean and consistent
-   [x] Data types are appropriate for each variable
-   [x] Missing values are identified and handled
-   [x] Outliers are investigated and documented
-   [x] Categorical variables are properly encoded
-   [x] Duplicate rows are checked and removed if needed
-   [x] Date/time variables are in proper format
-   [x] Cleaned data is saved for reproducibility
:::